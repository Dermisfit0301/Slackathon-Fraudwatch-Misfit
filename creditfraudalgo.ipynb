{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCjydzxnDQms"
      },
      "source": [
        "**Fraud transaction detection**\n",
        "\n",
        "We are trying to create an algorithm that detects a fraud transaction. Fraud transaction is the one that is not made by the user and is made by a phisher who has obtained the credentials by using sneaky methods.\n",
        "We have been provided with a dataset obtained from a bank.\n",
        "\n",
        "We are going to be studying the historical data in order to find patterns that indicate a fraud. \n",
        "Then we will apply machine learning techniques to create a model that will identify a fraud. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoPJ8-lrnlCV"
      },
      "source": [
        "**STEP 1:Setting up the environment**\n",
        "\n",
        "\n",
        "1.   **Packages**\n",
        "\n",
        "    We will import libraries from python server which will help us with the analysis.\n",
        "    statsmodels is a Python module that provides classes and functions for the estimation of many different statistical models\n",
        "\n",
        "    Pandas used for data analysis and manipulation\n",
        "\n",
        "    Numpy for mathematical formulae and stuff like that.\n",
        "\n",
        "    Seaborn for data visualization\n",
        "\n",
        "    Matplotlib will help us visualize the data\n",
        "\n",
        "    Scipy for statistical functions\n",
        "\n",
        "    Sklearn package will help us with Machine learning Ipywidgets for plotting.\n",
        "    \n",
        "    Date time for date and time ops\n",
        "    \n",
        "    Xgboost gives us their classifier\n",
        "\n",
        "\n",
        "2.   **Loading the dataset**\n",
        "  \n",
        "  We will get the dataset from the google drive after mounting the drive on the system.Then we read the csv file. \n",
        "\n",
        "3.   **Transforming data**\n",
        "\n",
        "  The dataset is in two parts. We merge the two datasets and create a single dataset fro better analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kJtq-SOHkdp8"
      },
      "outputs": [],
      "source": [
        "#importing packages\n",
        "from pandas import Series;  from numpy.random import randn\n",
        "from statsmodels.stats.weightstats import ttest_ind\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import ttest_ind\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.stats import shapiro\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression as lgr\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import preprocessing as preproc\n",
        "from sklearn import metrics\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Awh7-dQPfB-8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, auc, balanced_accuracy_score, confusion_matrix, f1_score, precision_score, average_precision_score, roc_auc_score,  recall_score,  precision_recall_curve as skm\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest, RandomForestClassifier \n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate, train_test_split \n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV \n",
        "from hyperopt import hp, tpe, STATUS_OK, fmin, Trials \n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "metadata": {
        "id": "nnsU_Vvp7FEW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WF-a9blm9k3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4a6a15-727c-48b3-94f5-9a33ce673203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap\n"
      ],
      "metadata": {
        "id": "rxrg-cXBgRsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09af9c7-ee1f-4422-b4d6-81e807a58253"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting shap\n",
            "  Downloading shap-0.41.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (569 kB)\n",
            "\u001b[K     |████████████████████████████████| 569 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.56.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.21.6)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.7/dist-packages (from shap) (21.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.7.3)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.64.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>20.9->shap) (3.0.9)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->shap) (4.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.39.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->shap) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->shap) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.2.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.41.0 slicer-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shap"
      ],
      "metadata": {
        "id": "Qx4g72W4hMEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLESBqVa-uoP"
      },
      "outputs": [],
      "source": [
        "#getting the path\n",
        "path1=\"/content/drive/MyDrive/fraudTest.csv\"\n",
        "path2=\"/content/drive/MyDrive/fraudTrain.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biBs5gtADOQU"
      },
      "outputs": [],
      "source": [
        "#part 1`\n",
        "part1=pd.read_csv('/content/drive/MyDrive/fraudTest.csv')\n",
        "part1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7s-lbcQHLKz"
      },
      "outputs": [],
      "source": [
        "#part 2\n",
        "part2=pd.read_csv('/content/drive/MyDrive/fraudTrain.csv')\n",
        "part2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RB3b5m5IASA"
      },
      "outputs": [],
      "source": [
        "#merger\n",
        "credtransdf=pd.concat([part1,part2])\n",
        "credtransdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QhdDYXBHvkL"
      },
      "source": [
        "**Step 2: Data cleaning and organizing**\n",
        "\n",
        "In this step, we are going to be \"cleaning\" the data. We are going to remove the unnecessary columns and deal with missing values. We will also remove outliers in the dataset and make the data fit for analysis. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1nbgSsmJ05M"
      },
      "outputs": [],
      "source": [
        "#seeing the shape\n",
        "credtransdf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpSc218qI94P"
      },
      "outputs": [],
      "source": [
        "#seeing the datatypes\n",
        "credtransdf.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-F-beA5KsxS"
      },
      "outputs": [],
      "source": [
        "#replacing missing values with nan(Not a number) that makes things simpler for python\n",
        "credtransdf.replace(\"?\",np.nan,inplace = True)\n",
        "print(credtransdf.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve6pOWtKLwJx"
      },
      "outputs": [],
      "source": [
        "#seeing the missing data\n",
        "missing_data=credtransdf.isnull()\n",
        "missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjtgEvHSMAzH"
      },
      "outputs": [],
      "source": [
        "#let us see the number of missing values in each column\n",
        "for column in missing_data.columns.values.tolist():\n",
        "  print(column,missing_data[column].value_counts())\n",
        "  print(\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoRyr6nRgrI7"
      },
      "outputs": [],
      "source": [
        "#Inter quartile range for amt \n",
        "for x in ['amt']:\n",
        "    q75,q25 = np.percentile(credtransdf.loc[:,x],[75,25])\n",
        "    intr_qr = q75-q25\n",
        "intr_qr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcDpHQ7yjlEE"
      },
      "outputs": [],
      "source": [
        "#defining upper and lower bound\n",
        "max = q75+(1.5*intr_qr)\n",
        "min = q25-(1.5*intr_qr)\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63dzMhFNkqfw"
      },
      "outputs": [],
      "source": [
        "#locating outliers and making them nan\n",
        "credtransdf.loc[credtransdf[x] < min,x] = np.nan\n",
        "credtransdf.loc[credtransdf[x] > max,x] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMWnRRAklMvP"
      },
      "outputs": [],
      "source": [
        "#seeing null values\n",
        "credtransdf.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrZuZmDxlYeP"
      },
      "outputs": [],
      "source": [
        "#dropping the null values\n",
        "credtransdf = credtransdf.dropna(axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqjTEK1WLy_9"
      },
      "outputs": [],
      "source": [
        "#boxplot test\n",
        "sb.boxplot(credtransdf['amt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpmGG5DAbFCA"
      },
      "outputs": [],
      "source": [
        "#Inter quartile range for lat \n",
        "for x in ['lat']:\n",
        "    q75,q25 = np.percentile(credtransdf.loc[:,x],[75,25])\n",
        "    intr_qr = q75-q25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78kBsp8Ub9ON"
      },
      "outputs": [],
      "source": [
        "#defining upper and lower bound\n",
        "max = q75+(1.5*intr_qr)\n",
        "min = q25-(1.5*intr_qr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwGJH3YCcBZj"
      },
      "outputs": [],
      "source": [
        "#locating outliers and making them nan\n",
        "credtransdf.loc[credtransdf[x] < min,x] = np.nan\n",
        "credtransdf.loc[credtransdf[x] > max,x] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFl1ABpncD1j"
      },
      "outputs": [],
      "source": [
        "#seeing null values\n",
        "credtransdf.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9s8dxl9cGIU"
      },
      "outputs": [],
      "source": [
        "#dropping the null values\n",
        "credtransdf = credtransdf.dropna(axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJq3z0fNcH5M"
      },
      "outputs": [],
      "source": [
        "#boxplot test\n",
        "sb.boxplot(credtransdf['lat'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS6t1tnBcMdc"
      },
      "outputs": [],
      "source": [
        "#Inter quartile range for long \n",
        "for x in ['long']:\n",
        "    q75,q25 = np.percentile(credtransdf.loc[:,x],[75,25])\n",
        "    intr_qr = q75-q25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XKNsi92cWJL"
      },
      "outputs": [],
      "source": [
        "#defining upper and lower bound\n",
        "max = q75+(1.5*intr_qr)\n",
        "min = q25-(1.5*intr_qr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRbumVGOcf-k"
      },
      "outputs": [],
      "source": [
        "#locating outliers and making them nan\n",
        "credtransdf.loc[credtransdf[x] < min,x] = np.nan\n",
        "credtransdf.loc[credtransdf[x] > max,x] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQQUqJpVciZD"
      },
      "outputs": [],
      "source": [
        "#seeing null values\n",
        "credtransdf.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nfLXpWjcq4L"
      },
      "outputs": [],
      "source": [
        "#dropping the null values\n",
        "credtransdf = credtransdf.dropna(axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XXTskoLcsbD"
      },
      "outputs": [],
      "source": [
        "#boxplot test\n",
        "sb.boxplot(credtransdf['long'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-bR-UMVdkj5"
      },
      "outputs": [],
      "source": [
        "#Inter quartile range for merch lat\n",
        "for x in ['merch_lat']:\n",
        "    q75,q25 = np.percentile(credtransdf.loc[:,x],[75,25])\n",
        "    intr_qr = q75-q25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBrAoaCJefjU"
      },
      "outputs": [],
      "source": [
        "#defining upper and lower bound\n",
        "max = q75+(1.5*intr_qr)\n",
        "min = q25-(1.5*intr_qr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AsgCmsIehBo"
      },
      "outputs": [],
      "source": [
        "#locating outliers and making them nan\n",
        "credtransdf.loc[credtransdf[x] < min,x] = np.nan\n",
        "credtransdf.loc[credtransdf[x] > max,x] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olK0LfTUeiK3"
      },
      "outputs": [],
      "source": [
        "#seeing null values\n",
        "credtransdf.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTVxOvWVekMW"
      },
      "outputs": [],
      "source": [
        "#dropping the null values\n",
        "credtransdf = credtransdf.dropna(axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTUTUNruel9m"
      },
      "outputs": [],
      "source": [
        "#boxplot test\n",
        "sb.boxplot(credtransdf['merch_lat'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZJ1AD8Jewkm"
      },
      "outputs": [],
      "source": [
        "#Inter quartile range for merch_long  \n",
        "for x in ['merch_long']:\n",
        "    q75,q25 = np.percentile(credtransdf.loc[:,x],[75,25])\n",
        "    intr_qr = q75-q25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6tZe4ulfryk"
      },
      "outputs": [],
      "source": [
        "#defining upper and lower bound\n",
        "max = q75+(1.5*intr_qr)\n",
        "min = q25-(1.5*intr_qr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxsVh2buftRE"
      },
      "outputs": [],
      "source": [
        "#locating outliers and making them nan\n",
        "credtransdf.loc[credtransdf[x] < min,x] = np.nan\n",
        "credtransdf.loc[credtransdf[x] > max,x] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq9kNQBcgP_r"
      },
      "outputs": [],
      "source": [
        "#seeing null values\n",
        "credtransdf.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPvnQRo1gRLL"
      },
      "outputs": [],
      "source": [
        "#dropping the null values\n",
        "credtransdf = credtransdf.dropna(axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn25rtSEgS4z"
      },
      "outputs": [],
      "source": [
        "#boxplot test\n",
        "sb.boxplot(credtransdf['merch_long'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "506UAP_oEdZj"
      },
      "outputs": [],
      "source": [
        "#Inter quartile range for merch lat\n",
        "for x in ['city_pop']:\n",
        "    q75,q25 = np.percentile(credtransdf.loc[:,x],[75,25])\n",
        "    intr_qr = q75-q25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeNpClfrE7_y"
      },
      "outputs": [],
      "source": [
        "#defining upper and lower bound\n",
        "max = q75+(1.5*intr_qr)\n",
        "min = q25-(1.5*intr_qr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dUCb_hpE_lY"
      },
      "outputs": [],
      "source": [
        "#locating outliers and making them nan\n",
        "credtransdf.loc[credtransdf[x] < min,x] = np.nan\n",
        "credtransdf.loc[credtransdf[x] > max,x] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaFtWoxlFBGF"
      },
      "outputs": [],
      "source": [
        "#seeing null values\n",
        "credtransdf.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbdnjDZ0FDJY"
      },
      "outputs": [],
      "source": [
        "#dropping the null values\n",
        "credtransdf = credtransdf.dropna(axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQxjhGu9FE4Y"
      },
      "outputs": [],
      "source": [
        "#boxplot test\n",
        "sb.boxplot(credtransdf['city_pop'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6gYZolUeo6X"
      },
      "source": [
        "**Step 3: Feature engineering**\n",
        "\n",
        "In this step we are going to create new features which will help us in understanding the data better.\n",
        "We will create bins for complex numerical or time based variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frnqg--PAFrU"
      },
      "outputs": [],
      "source": [
        "#seperating year from dob column\n",
        "bornyr = credtransdf[\"dob\"].str.split(\"-\", n = 1, expand = True)\n",
        "bornyr[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiXSQbFeMeJn"
      },
      "outputs": [],
      "source": [
        "#checking data type\n",
        "print(type(bornyr[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUFMRxYaNkZ8"
      },
      "outputs": [],
      "source": [
        "#changing data type\n",
        "bornyr[0]=bornyr[0].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8Fd2FG8Jvel"
      },
      "outputs": [],
      "source": [
        "#introducing age column in database\n",
        "today= date.today()\n",
        "age=today.year-bornyr[0]\n",
        "age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLHAtNx_OJcB"
      },
      "outputs": [],
      "source": [
        "#adding column to the dataset\n",
        "credtransdf[\"Age\"]=age\n",
        "credtransdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgGi1P4tKZVF"
      },
      "source": [
        "type("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ0wASETJl9i"
      },
      "outputs": [],
      "source": [
        "credtransdf[\"Age\"]=credtransdf[\"Age\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr9bkkD3LBfe"
      },
      "outputs": [],
      "source": [
        "credtransdf[\"Age\"].min()\n",
        "credtransdf[\"Age\"].max()\n",
        "print(credtransdf[\"Age\"].min())\n",
        "print(credtransdf[\"Age\"].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrE6_St8YTGP"
      },
      "outputs": [],
      "source": [
        "#fitting the variable\n",
        "credtransdf.Age=credtransdf['Age']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQY-4radWr9C"
      },
      "outputs": [],
      "source": [
        "#creating bins for Age\n",
        "bins = [16, 25, 40, 60, 100]\n",
        "\n",
        "# add custom labels if desired\n",
        "labels = ['Youth', 'Adult', 'Middle Age', 'Senior citizen']\n",
        "\n",
        "# add the bins to the dataframe\n",
        "credtransdf['Age Category'] = pd.cut(credtransdf.Age, bins, labels=labels, right=False)\n",
        "\n",
        "credtransdf['Age Category']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lagVBkJIchsa"
      },
      "outputs": [],
      "source": [
        "#seperating trans time from trans date column\n",
        "trans_time = credtransdf[\"trans_date_trans_time\"].str.split(\" \", n = 1, expand = True)\n",
        "trans_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHrEFW2kvFbR"
      },
      "outputs": [],
      "source": [
        "#creating new columns\n",
        "credtransdf['Trans_date']=trans_time[0]\n",
        "credtransdf['Trans_time']=trans_time[1]\n",
        "credtransdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry6K3ETaTdzi"
      },
      "outputs": [],
      "source": [
        "#changing the datatype\n",
        "credtransdf.time= pd.to_datetime(credtransdf['Trans_time'])\n",
        "credtransdf.time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKtWYvSb2UIQ"
      },
      "outputs": [],
      "source": [
        "#creating bins for Trans time\n",
        "bins = [0, 6, 12, 18, 24]\n",
        "\n",
        "# add custom labels if desired\n",
        "labels = ['Night hours', 'Morning hours', 'Afternoon hours', 'Evening hours']\n",
        "\n",
        "# add the bins to the dataframe\n",
        "credtransdf['Part of day'] = pd.cut(credtransdf.time.dt.hour, bins, labels=labels, right=False)\n",
        "\n",
        "credtransdf['Part of day']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "credtransdf.head(0)"
      ],
      "metadata": {
        "id": "S8Z4Kp_sFwb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLDEzIVTysDW"
      },
      "outputs": [],
      "source": [
        "#dropping unnecessary columns\n",
        "credtransdf.drop(['Unnamed: 0','cc_num','first','last','street','zip','unix_time','trans_num','trans_date_trans_time'], axis=1, inplace=True)\n",
        "credtransdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df_8-z5UhSbR"
      },
      "source": [
        "**Step 4: Exploratory Data analysis**\n",
        "\n",
        "Here we are going to know about our data. We will run hypothesis tests in this segment. We will use graphics and statistics to understand the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz0NCjFT4TJO"
      },
      "outputs": [],
      "source": [
        "#summary stats\n",
        "credtransdf.describe().round()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQo_ymOh-4eN"
      },
      "outputs": [],
      "source": [
        "#seeing correlation\n",
        "credtransdf.corr()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD5dBXoOGfef"
      },
      "outputs": [],
      "source": [
        "#correlation heatmap\n",
        "sb.heatmap(credtransdf.corr(), annot=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jcd9akuiH9a"
      },
      "outputs": [],
      "source": [
        "#seeing the number of fraud transactions\n",
        "credtransdf['is_fraud'].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUbFb3TckUh6"
      },
      "outputs": [],
      "source": [
        "#assigning variables to classes\n",
        "fraud=credtransdf[credtransdf.is_fraud == True]\n",
        "genuine=credtransdf[credtransdf.is_fraud == False]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ktQf63NmevF"
      },
      "outputs": [],
      "source": [
        "#unbalanced dataset(as 1 in 1000 transactions are fraud)\n",
        "percentfraud=len(fraud)/(len(fraud)+len(genuine))*100\n",
        "percentfraud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPaAwO6ec5SA"
      },
      "outputs": [],
      "source": [
        "#subbsetting fraud for time series\n",
        "amtlostts=fraud[[\"amt\",\"Trans_date\"]]\n",
        "amtlostts = pd.DataFrame(fraud, columns = ['Trans_date', 'amt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O3rpRvyYtgw"
      },
      "outputs": [],
      "source": [
        "# Set the Date as Index\n",
        "amtlostts['Trans_date'] = pd.to_datetime(amtlostts['Trans_date'])\n",
        "amtlostts.index = amtlostts['Trans_date']\n",
        "del amtlostts['Trans_date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyBhLZSlY29w"
      },
      "outputs": [],
      "source": [
        "#time series for amount lost to fraud transactions during the time period in focus\n",
        "amtlostts.plot(figsize=(20, 4))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsZdT9TjJice"
      },
      "outputs": [],
      "source": [
        "#setting values for x and y\n",
        "Age_cat=fraud['Age Category'].unique()\n",
        "cases=fraud['Age Category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4mPtPRnMkFI"
      },
      "outputs": [],
      "source": [
        "#visualization stage\n",
        "plt.bar(Age_cat, cases)\n",
        "plt.xlabel(\"Age groups\")\n",
        "plt.ylabel(\"Number of cases of fraud transactions\")\n",
        "plt.title(\"Amount of fraud transactions per Age category\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqFNg24I7YTJ"
      },
      "outputs": [],
      "source": [
        "#setting values for x and y\n",
        "cat=fraud['category'].unique()\n",
        "occurence=fraud['category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Soyvr0-76opK"
      },
      "outputs": [],
      "source": [
        "#plotting fraud cases per spending category\n",
        "plt.plot(cat,occurence)\n",
        "plt.title('Fraud cases per category of spending')\n",
        "plt.xlabel('category names')\n",
        "plt.ylabel('number of cases')\n",
        "plt.rcParams['figure.figsize'] = [17, 7]\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4HlfdRGdV85"
      },
      "outputs": [],
      "source": [
        "#checking distribution of amt\n",
        "stat, p = shapiro(credtransdf['amt'])\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "\tprint('Probably Gaussian')\n",
        "else:\n",
        "\tprint('Probably not Gaussian')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROIPp-aolS8m"
      },
      "outputs": [],
      "source": [
        "#checking distribution of amt\n",
        "stat, p = shapiro(credtransdf['city_pop'])\n",
        "print('stat=%.3f, p=%.3f' % (stat, p))\n",
        "if p > 0.05:\n",
        "\tprint('Probably Gaussian')\n",
        "else:\n",
        "\tprint('Probably not Gaussian')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AJghBYdoChn"
      },
      "outputs": [],
      "source": [
        "#setting values for x and y\n",
        "partofday=fraud['Part of day'].unique()\n",
        "incidents=fraud['Part of day'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa44vJULn-lL"
      },
      "outputs": [],
      "source": [
        "#what part of day you are likely to see a fraud transaction\n",
        "plt.pie(incidents, labels = partofday, startangle = 75,autopct='%.0f%%')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt4KX1Yo3OWc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#amount of money each state loses to fraud by anova\n",
        "mod1 = ols(\"amt ~ state\", data = fraud).fit()\n",
        "anov_table1 = sm.stats.anova_lm(mod1)\n",
        "anov_table1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3lrmtEACp_4"
      },
      "outputs": [],
      "source": [
        "#amount of money each state loses to fraud by anova\n",
        "mod1 = ols(\"amt ~ state\", data = fraud).fit()\n",
        "anov_table1 = sm.stats.anova_lm(mod1)\n",
        "anov_table1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9fP8p4pDKrv"
      },
      "outputs": [],
      "source": [
        "#amount of money each age category loses to fraud by anova\n",
        "agecat=fraud['Age Category']\n",
        "mod2 = ols(\"amt ~ agecat\", data = fraud).fit()\n",
        "anov_table2 = sm.stats.anova_lm(mod2)\n",
        "anov_table2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMP-BYeoDvEg"
      },
      "outputs": [],
      "source": [
        "#amount of money each part of day loses to fraud by anova\n",
        "timebin=fraud['Part of day']\n",
        "mod3 = ols(\"amt ~ timebin\", data = fraud).fit()\n",
        "anov_table3 = sm.stats.anova_lm(mod3)\n",
        "anov_table3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIxp89dl1Feh"
      },
      "outputs": [],
      "source": [
        "#amount of money each part of day loses to fraud by anova\n",
        "mod4 = ols(\"amt ~ category\", data = fraud).fit()\n",
        "anov_table4 = sm.stats.anova_lm(mod4)\n",
        "anov_table4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXAz4ieyQ1Uv"
      },
      "outputs": [],
      "source": [
        "#assigning variables to classes\n",
        "Male=fraud[fraud.gender == 'M']\n",
        "Female=fraud[fraud.gender == 'F']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7CBwHSidy1x"
      },
      "outputs": [],
      "source": [
        "#checking if amount loss is same between two genders\n",
        "stats.ttest_ind(Male['amt'], Female['amt'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQS_OyKilJgO"
      },
      "source": [
        "**Conclusion**\n",
        "\n",
        "\n",
        "\n",
        "*   We found out gender,time of the day has a significant impact on amount of money lost\n",
        "*   Senior citizens are the most likely to lose money during fraud transactions. \n",
        "\n",
        "*   Evening is the time where most fraud transactions take place.\n",
        "*  All states lose almost the same amount due to fraud transactions.\n",
        "\n",
        "*   The customers are are most likely to lose money when spending on health or fitness.\n",
        "*   It is women who lose more amount of money to fraudulent transactions than men.\n",
        "\n",
        "\n",
        "*  The fraud amount was higher before the lockdown period.\n",
        "*   The fraud amount dipped in the lockdown period and peaked again in 2021 when the lockdown ended.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9fCiIZzrrLC"
      },
      "source": [
        "**Step 5: Model building**\n",
        "\n",
        "We will use machine learning techniques to build our model which will help us detect a fraud transaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzGIsxJMdZ6W"
      },
      "outputs": [],
      "source": [
        "#taking subset of genuine and fraud\n",
        "genprt=genuine[0:1800]\n",
        "fraudprt=fraud[0:1800]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYSxxG_eyTtO"
      },
      "outputs": [],
      "source": [
        "#creating a subset where fraud and genuine cases are in equal number \n",
        "cdtsubset=pd.concat([genprt,fraudprt])\n",
        "cdtsubset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4d52y2tOj4j"
      },
      "outputs": [],
      "source": [
        "#we will do the pca (Principal component analysis) after we define the important components first\n",
        "#standardization defining\n",
        "standardized=StandardScaler()\n",
        "#pca setup\n",
        "pca=PCA()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unmDhImDJlNA"
      },
      "outputs": [],
      "source": [
        "#making dataset pca fit\n",
        "#dropping unncecessary variables\n",
        "cdtsubsetnum=cdtsubset.drop(['merchant','category','city','state','job','dob','is_fraud','Age Category','Trans_date','Trans_time','Part of day','gender'], axis=1)\n",
        "cdtsubsetnum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "011SzWneKR2F"
      },
      "outputs": [],
      "source": [
        "#fitting the model\n",
        "standardized.fit(cdtsubsetnum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1Z0b6kNKnmc"
      },
      "outputs": [],
      "source": [
        "#scaled data creation\n",
        "scaled_data=pd.DataFrame(standardized.transform(cdtsubsetnum))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI3xFqy-Ks-3"
      },
      "outputs": [],
      "source": [
        "#viewing the new dataset\n",
        "scaled_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppYnt50RKwgV"
      },
      "outputs": [],
      "source": [
        "#fitting scaled data\n",
        "pca.fit(scaled_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbOvEZaPKzO7"
      },
      "outputs": [],
      "source": [
        "#variance ratio\n",
        "vr=pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXg1VLmPK1s8"
      },
      "outputs": [],
      "source": [
        "#cumulative variance\n",
        "cum_vr=np.cumsum(vr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkVH74e1K3KT"
      },
      "outputs": [],
      "source": [
        "#plotting the cumulative sum\n",
        "plt.plot(cum_vr)\n",
        "plt.rcParams['figure.figsize'] = [5, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMqfk5OiK7G9"
      },
      "outputs": [],
      "source": [
        "#scree plot\n",
        "per_var=np.round(vr*100)\n",
        "labels= ['PC'+str(x) for x in range(1,len(per_var)+1)]\n",
        "plt.bar(x=range(1,len(per_var)+1),height=per_var, tick_label=labels)\n",
        "plt.xlabel(\"Principal component\")\n",
        "plt.ylabel(\"Percentage of variance explained\")\n",
        "plt.title(\"Scree plot\")\n",
        "plt.show()\n",
        "plt.rcParams['figure.figsize'] = [5, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhS1PTGeOqiU"
      },
      "outputs": [],
      "source": [
        "#loading series for 1\n",
        "loading_scores=pd.Series(pca.components_[0])\n",
        "sorted_load_score=loading_scores.abs().sort_values(ascending=False)\n",
        "top3=sorted_load_score[0:3].index.values\n",
        "top3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLSxi8qSOqny"
      },
      "outputs": [],
      "source": [
        "#loading series for 2\n",
        "loading_scores=pd.Series(pca.components_[1])\n",
        "sorted_load_score=loading_scores.abs().sort_values(ascending=False)\n",
        "top3=sorted_load_score[0:3].index.values\n",
        "top3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPJuIRw0U4ds"
      },
      "outputs": [],
      "source": [
        "#loading series for 3\n",
        "loading_scores=pd.Series(pca.components_[2])\n",
        "sorted_load_score=loading_scores.abs().sort_values(ascending=False)\n",
        "top3=sorted_load_score[0:3].index.values\n",
        "top3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5cilvVEVBLs"
      },
      "outputs": [],
      "source": [
        "scaled_data.head(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLq0if8gVIdD"
      },
      "outputs": [],
      "source": [
        "cdtsubsetnum.head(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzU7wI-39u3W"
      },
      "outputs": [],
      "source": [
        "cdtsubset.head(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "We found out that the first 4 Principal components have equal weightage, and hence the model wont be effective. So we have to look for other ways to build a model. If we cant find 2 major principal components to describe the model then the whole purpose of the exercise is defeated."
      ],
      "metadata": {
        "id": "elP05QTNdlkA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xtjeIsLowXI"
      },
      "outputs": [],
      "source": [
        "#checking for duplicate values\n",
        "cdtsubset.index.is_unique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZfBnI_opBLP"
      },
      "outputs": [],
      "source": [
        "#seeing duplicates\n",
        "cdtsubset.index.duplicated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ1HU-lZpL4f"
      },
      "outputs": [],
      "source": [
        "#Drop rows with duplicate index values\n",
        "cdtsubset=cdtsubset.loc[~cdtsubset.index.duplicated(), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rreAG37erb4R"
      },
      "outputs": [],
      "source": [
        "#seeing if duplicate is gone\n",
        "cdtsubset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxOtoC3kpTV4"
      },
      "outputs": [],
      "source": [
        "#Prevent duplicate values in a DataFrame index\n",
        "cdtsubset.flags.allows_duplicate_labels = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2mAvUmZptxt"
      },
      "outputs": [],
      "source": [
        "#doing same for columns\n",
        "cdtsubset.columns.is_unique\n",
        "cdtsubset.columns.duplicated()\n",
        "cdtsubset.loc[:, ~cdtsubset.columns.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj32DZbqdGQ8"
      },
      "outputs": [],
      "source": [
        "#initalizing encoder\n",
        "label_encoder=preproc.LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYq7r4RCd6Zc"
      },
      "outputs": [],
      "source": [
        "#starting with gender\n",
        "encgen=label_encoder.fit_transform(cdtsubset[\"gender\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Few0nmaGfFSv"
      },
      "outputs": [],
      "source": [
        "#initializing lgr\n",
        "lgmodel=lgr(solver='lbfgs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4OS2vKIgYEP"
      },
      "outputs": [],
      "source": [
        "#trial run with one variable\n",
        "lgmodel.fit(X = pd.DataFrame(encgen),\n",
        "            y = cdtsubset['is_fraud'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9oDgRp9hGgx"
      },
      "outputs": [],
      "source": [
        "#check model intercept and co ef\n",
        "print(lgmodel.intercept_)\n",
        "print(lgmodel.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPX-RMMHhR-C"
      },
      "outputs": [],
      "source": [
        "#prediction\n",
        "preds=lgmodel.predict_proba(X = pd.DataFrame(encgen))\n",
        "preds=pd.DataFrame(preds)\n",
        "preds.columns=['genuine transaction','fraud transaction']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NXYajYAnBb1"
      },
      "outputs": [],
      "source": [
        "#generate table of predication against gender\n",
        "pd.crosstab(cdtsubset[\"gender\"], preds.loc[:,'fraud transaction'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzFa3yrltCrF"
      },
      "outputs": [],
      "source": [
        "cdtsubset.head(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jX1Yojfs_Zy"
      },
      "outputs": [],
      "source": [
        "#scaling the model up\n",
        "encgen=label_encoder.fit_transform(cdtsubset[\"gender\"])\n",
        "encmer=label_encoder.fit_transform(cdtsubset[\"merchant\"])\n",
        "enccat=label_encoder.fit_transform(cdtsubset[\"category\"])\n",
        "enccit=label_encoder.fit_transform(cdtsubset[\"city\"])\n",
        "encsta=label_encoder.fit_transform(cdtsubset[\"state\"])\n",
        "enctda=label_encoder.fit_transform(cdtsubset[\"Trans_date\"])\n",
        "encttm=label_encoder.fit_transform(cdtsubset[\"Trans_time\"])\n",
        "encagc=label_encoder.fit_transform(cdtsubset[\"Age Category\"])\n",
        "enctmb=label_encoder.fit_transform(cdtsubset[\"Part of day\"])\n",
        "encdob=label_encoder.fit_transform(cdtsubset[\"dob\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwQQ8Ba6uxHa"
      },
      "outputs": [],
      "source": [
        "#setup for model building\n",
        "train_features=pd.DataFrame([encgen,\n",
        "                             encmer,\n",
        "                             enccat,\n",
        "                             enccit,\n",
        "                             encsta,\n",
        "                             enctda,\n",
        "                             encttm,\n",
        "                             encagc,\n",
        "                             enctmb,\n",
        "                             encdob,\n",
        "                             cdtsubset['Age'],\n",
        "                             cdtsubset['amt'],\n",
        "                             cdtsubset['lat'],\n",
        "                             cdtsubset['long'],\n",
        "                             cdtsubset['city_pop'],\n",
        "                             cdtsubset['merch_lat'],\n",
        "                             cdtsubset['merch_long']]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0kFVuhJx0CL"
      },
      "outputs": [],
      "source": [
        "#initialize model\n",
        "logmodel=lgr(solver = 'lbfgs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9icfCvYyeN_M"
      },
      "outputs": [],
      "source": [
        "#fitting variables \n",
        "train=train_features\n",
        "len(train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generate table of predication against gender\n",
        "pd.crosstab(cdtsubset[\"gender\"], preds.loc[:,'fraud transaction'])"
      ],
      "metadata": {
        "id": "Lh7dpTPqgeFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY2Banq-Jnl9"
      },
      "outputs": [],
      "source": [
        "#Drop rows with duplicate index values\n",
        "cdtsubsetnum=cdtsubsetnum.loc[~cdtsubsetnum.index.duplicated(), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRSaYZu2uXnC"
      },
      "outputs": [],
      "source": [
        "#seeing if its gone\n",
        "cdtsubsetnum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'X are independent variables and Y is dependent variable or class we are trying to label'\n",
        "#setting values of X and Y \n",
        "X=train\n",
        "y= cdtsubset['is_fraud']"
      ],
      "metadata": {
        "id": "sXKQUAKFOk-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku0cakxebsLN"
      },
      "outputs": [],
      "source": [
        "#fitting the model\n",
        "X, y = make_classification(random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state=42)\n",
        "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
        "pipe.fit(X_train, y_train)  # apply scaling on training data\n",
        "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
        "                ('logisticregression', LogisticRegression())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knah22OSbwpD"
      },
      "outputs": [],
      "source": [
        "#initializing scaler\n",
        "scaler = preproc.StandardScaler().fit(X_train)\n",
        "scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO--PMX1ggZi"
      },
      "outputs": [],
      "source": [
        "#printing mean and scale\n",
        "print(scaler.mean_)\n",
        "print(scaler.scale_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Seecre7hiEZb"
      },
      "outputs": [],
      "source": [
        "#transforming compnent X\n",
        "X_scaled = scaler.transform(X_train)\n",
        "len(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PItV7dYN4G-r"
      },
      "outputs": [],
      "source": [
        "#setting dataset\n",
        "logmodel.fit(X= X_scaled,\n",
        "             y=y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGpyvlZdedTG"
      },
      "outputs": [],
      "source": [
        "#check model intercept and co ef\n",
        "print(logmodel.intercept_)\n",
        "print(logmodel.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZeKbYLRkmk-"
      },
      "outputs": [],
      "source": [
        "#seeing mean and sd of X\n",
        "print(X_scaled.mean(axis=0))\n",
        "print(X_scaled.std(axis=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aIYaJLule5L"
      },
      "outputs": [],
      "source": [
        "#initializing pipe\n",
        "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
        "pipe.fit(X_train, y_train)  # apply scaling on training data\n",
        "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
        "                ('logisticregression', LogisticRegression())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pghJO1LBlp-L"
      },
      "outputs": [],
      "source": [
        "#checking pipe score\n",
        "pipe.score(X_test, y_test) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#making function to evaluate model.\n",
        "def evaluate_model(X, y, model):\n",
        "\t# define evaluation procedure\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\t# define the model evaluation the metric\n",
        "\tmetric = make_scorer(pr_auc, needs_proba=True)\n",
        "\t# evaluate model\n",
        "\tscores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
        "\tstatement= print('Mean PR AUC: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "\treturn statement\n",
        "\n"
      ],
      "metadata": {
        "id": "aJM2IYQGEalK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate precision-recall area under curve\n",
        "def pr_auc(y_true, probas_pred):\n",
        "\t# calculate precision-recall curve\n",
        "\tp, r, _ = precision_recall_curve(y_true, probas_pred)\n",
        "\t# calculate area under curve\n",
        "\treturn auc(r, p)"
      ],
      "metadata": {
        "id": "v_DloVls_pOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TerqDwgYEIx-"
      },
      "outputs": [],
      "source": [
        "\"Naive Bayes Classifier\"\n",
        "#setting the terms for the first model\n",
        "model1=GaussianNB()\n",
        "model1.fit(X_train, y_train)\n",
        "Y_pred1 = model1.predict(X_test)\n",
        "\n",
        "#printing metrics\n",
        "print(classification_report(y_test,Y_pred1  ))\n",
        "print(\"F1 score is:\",f1_score(y_test,Y_pred1  ))\n",
        "print(\"The Predictions are:\",Y_pred1 )\n",
        "print(\"AUC score is:\",pr_auc(y_test,Y_pred1 ))\n",
        "evaluate_model(X, y,model1)\n",
        "\n",
        "#prediction\n",
        "yhat1 = model1.predict_proba(X_test)\n",
        "result1= yhat1[0][1]\n",
        "print('>Predicted=%.3f (expected 0)' % (result1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6XxfpwrGkYB"
      },
      "outputs": [],
      "source": [
        "\"Random Forest Classifier\"\n",
        "#setting the terms for the second model\n",
        "model2=RandomForestClassifier(max_depth=45, n_estimators=500,n_jobs=-1,min_samples_leaf=200)\n",
        "#training dataset\n",
        "model2.fit(X_train,y_train)\n",
        "#testing dataset\n",
        "Y_pred2 = model2.predict(X_test)\n",
        "#printing metrics\n",
        "print(classification_report(y_test,Y_pred2 ))\n",
        "print(\"F1 score is:\",f1_score(y_test,Y_pred2 ))\n",
        "print(\"The Predictions are:\",Y_pred2)\n",
        "print(\"AUC score is:\",pr_auc(y_test,Y_pred2))\n",
        "evaluate_model(X, y, model2)\n",
        "#prediction\n",
        "yhat2 = model2.predict_proba(X_test)\n",
        "result2= yhat2[0][1]\n",
        "print('>Predicted=%.3f (expected 0)' % (result2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7zaDF_ecUTs"
      },
      "outputs": [],
      "source": [
        "\"XGB Classifier\"\n",
        "#setting the terms for the third model\n",
        "model3=XGBClassifier(n__jobs=-1)\n",
        "#training dataset\n",
        "model3.fit(X_train,y_train)\n",
        "#testing dataset\n",
        "Y_pred3 = model3.predict(X_test)\n",
        "#printing metrics\n",
        "print(classification_report(y_test,Y_pred3  ))\n",
        "print(\"F1 score is:\",f1_score(y_test,Y_pred3  ))\n",
        "print(\"The Predictions are:\",Y_pred3 )\n",
        "print(\"AUC score is:\",pr_auc(y_test,Y_pred3 ))\n",
        "evaluate_model(X, y,model3)\n",
        "\n",
        "#prediction\n",
        "yhat3 = model3.predict_proba(X_test)\n",
        "result3= yhat3[0][1]\n",
        "print('>Predicted=%.3f (expected 0)' % (result3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"Decision Trees Classifier\"\n",
        "#setting the terms for the fourth model\n",
        "model4 = DecisionTreeClassifier(max_depth=43)\n",
        "#training dataset\n",
        "model4.fit(X_train, y_train)\n",
        "#testing dataset\n",
        "Y_pred4 = model4.predict(X_test)\n",
        "#printing metrics\n",
        "print(classification_report(y_test,Y_pred4 ))\n",
        "print(\"F1 score is:\",f1_score(y_test,Y_pred4 ))\n",
        "print(\"The Predictions are:\",Y_pred4 )\n",
        "print(\"AUC score is:\",pr_auc(y_test,Y_pred4 ))\n",
        "evaluate_model(X, y,model4)\n",
        "\n",
        "#prediction\n",
        "yhat4 = model4.predict_proba(X_test)\n",
        "result4= yhat4[0][1]\n",
        "print('Predicted=%.3f (expected 0)' % (result4))"
      ],
      "metadata": {
        "id": "PRcXxAdVU7ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Support Vector Machine\"\n",
        "#setting the terms for the fifth model\n",
        "model5= Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
        "model5.fit(X_train, y_train)\n",
        "Y_pred5 = model5.predict(X_test)\n",
        "#printing metrics\n",
        "print(classification_report(y_test,Y_pred5 ))\n",
        "print(\"F1 score is:\",f1_score(y_test,Y_pred5 ))\n",
        "print(\"The Predictions are:\",Y_pred5 )\n",
        "print(\"AUC score is:\",pr_auc(y_test,Y_pred5))\n",
        "evaluate_model(X, y,model5)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HmRwFW9Qvpab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"K-Neighbors Classifier\"\n",
        "#setting the terms for the sixth model\n",
        "model6 = KNeighborsClassifier()\n",
        "model6= Pipeline([('scaler',StandardScaler()),('m',KNeighborsClassifier())])\n",
        "model6.fit(X_train, y_train)\n",
        "Y_pred6 = model6.predict(X_test)\n",
        "#printing metrics\n",
        "print(classification_report(y_test,Y_pred6 ))\n",
        "print(\"F1 score is:\",f1_score(y_test,Y_pred6 ))\n",
        "print(\"The Predictions are:\",Y_pred6 )\n",
        "print(\"AUC score is:\",pr_auc(y_test,Y_pred6))\n",
        "evaluate_model(X, y,model6)\n",
        "\n",
        "#prediction\n",
        "yhat6 = model6.predict_proba(X_test)\n",
        "result6= yhat6[0][1]\n",
        "print('>Predicted=%.3f (expected 0)' % (result6))"
      ],
      "metadata": {
        "id": "dM9HO0TRHl9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Bagging Classifier\"\n",
        "#setting the terms for the seventh model\n",
        "model7 = BaggingClassifier(n_estimators=100,n_jobs=-2)\n",
        "model7.fit(X_train, y_train)\n",
        "Y_pred7 = model7.predict(X_test)\n",
        "#printing metrics\n",
        "print(classification_report(y_test,Y_pred7 ))\n",
        "print(\"F1 score is:\",f1_score(y_test,Y_pred7 ))\n",
        "print(\"The Predictions are:\",Y_pred7 )\n",
        "print(\"AUC score is:\",pr_auc(y_test,Y_pred7))\n",
        "evaluate_model(X, y,model7)\n",
        "\n",
        "#prediction\n",
        "yhat7 = model7.predict_proba(X_test)\n",
        "result7 = yhat7 [0][1]\n",
        "print('>Predicted=%.3f (expected 0)' % (result7 ))"
      ],
      "metadata": {
        "id": "gnS10C-HMDwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Extra Trees Classifier\"\n",
        "#setting the terms for the eighth model\n",
        "model8 = ExtraTreesClassifier(max_depth=45, n_estimators=100,n_jobs=-1)\n",
        "model8.fit(X_train, y_train)\n",
        "Y_pred8 = model7.predict(X_test)\n",
        "#printing metrics\n",
        "print(classification_report(y_test,Y_pred8 ))\n",
        "print(\"F1 score is:\",f1_score(y_test,Y_pred8 ))\n",
        "print(\"The Predictions are:\",Y_pred8 )\n",
        "print(\"AUC score is:\",pr_auc(y_test,Y_pred8))\n",
        "evaluate_model(X, y,model8)\n",
        "\n",
        "#prediction\n",
        "yhat8 = model8.predict_proba(X_test)\n",
        "result8 = yhat8[0][1]\n",
        "print('>Predicted=%.3f (expected 0)' % (result8))"
      ],
      "metadata": {
        "id": "G9JKdroRNqj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Dummy Classifier\"\n",
        "#setting the terms for the ninth model\n",
        "model9 = DummyClassifier(strategy='constant', constant=1)\n",
        "model9.fit(X_train, y_train)\n",
        "Y_pred9 = model9.predict(X_test)\n",
        "#printing metrics\n",
        "print(classification_report(y_test,Y_pred9  ))\n",
        "print(\"F1 score is:\",f1_score(y_test,Y_pred9  ))\n",
        "print(\"The Predictions are:\",Y_pred9  )\n",
        "print(\"AUC score is:\",pr_auc(y_test,Y_pred9 ))\n",
        "evaluate_model(X, y,model9)\n",
        "#prediction\n",
        "yhat9 = model9.predict_proba(X_test)\n",
        "result9 = yhat9[0][1]\n",
        "print('>Predicted=%.3f (expected 0)' % (result9 ))"
      ],
      "metadata": {
        "id": "qLSDYngQz1k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Logistic Regression\"\n",
        "#setting the terms for the tenth model\n",
        "model10=LogisticRegression()\n",
        "#training dataset\n",
        "model10.fit(X_train,y_train)\n",
        "#testing dataset\n",
        "Y_pred10 = model10.predict(X_test)\n",
        "#printing metrics\n",
        "print(classification_report(y_test,Y_pred10))\n",
        "print(\"F1 score is:\",f1_score(y_test,Y_pred10))\n",
        "print(\"The Predictions are:\",Y_pred10)\n",
        "print(\"AUC score is:\",pr_auc(y_test,Y_pred10))\n",
        "evaluate_model(X, y, model10)\n",
        "#prediction\n",
        "yhat10 = model10.predict_proba(X_test)\n",
        "result10= yhat10[0][1]\n",
        "print('>Predicted=%.3f (expected 0)' % (result10))\n"
      ],
      "metadata": {
        "id": "x73_QiaNsflJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat11 = model10.predict_proba(X_scaled)\n",
        "result11= yhat11[0][1]\n",
        "print('>Predicted=%.3f (expected 0)' % (result10))"
      ],
      "metadata": {
        "id": "n6DFnW3rSqx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:** We will use the model of logistic regression as it is having the F score of 1 and AUC score of 1. That means the model is 100% accurate. That means the last model is the best for making an algorithm. "
      ],
      "metadata": {
        "id": "t3XaA_v3yoNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Making a basic algorithm**\n",
        "\n",
        "Here we are going to make a basic algorithm that will take 20 arrays and theresult is the probability of them being 0=Genuine or 1 = Fraud. If the probability of 0 is higher it is a genuine payment and if it is 1 then it is a fraudulent transaction.\n",
        "Since it is a basic algorithm it is going to take 20 arrays only. "
      ],
      "metadata": {
        "id": "_w67oGmQue3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result10 = yhat10[0][1]\n",
        "result10"
      ],
      "metadata": {
        "id": "5jmevkTirdhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#rounding up \n",
        "yhat10.round(0)"
      ],
      "metadata": {
        "id": "ZkwQsDm7EJ3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining fraudwatch function\n",
        "def fraudwatch(array):\n",
        "  for a,b in yhat10:\n",
        "    if a>b:\n",
        "      print(\"genuine\")\n",
        "    else:\n",
        "      print(\"fraud\")"
      ],
      "metadata": {
        "id": "ApFT00nfu7TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trial run\n",
        "ExperimentA=fraudwatch(yhat10)\n"
      ],
      "metadata": {
        "id": "LHEu4CWpFypk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(yhat10)"
      ],
      "metadata": {
        "id": "Pfl56QJe1ODv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}